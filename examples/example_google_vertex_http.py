#!/usr/bin/env python3
"""
Google Vertex AI via HTTP - à¸ªà¹ˆà¸‡ Native JSON Request à¹„à¸›à¸¢à¸±à¸‡ Proxy
à¸£à¸­à¸‡à¸£à¸±à¸š Thinking Config à¸ªà¸³à¸«à¸£à¸±à¸š Gemini 3.0
à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸•à¸£à¸µà¸¢à¸¡: Virtual Key à¸ˆà¸²à¸ Proxy, Proxy Server à¸•à¹‰à¸­à¸‡à¸£à¸±à¸™
"""

import requests
import json

def main():
    # URL à¸‚à¸­à¸‡ Proxy Server
    url = "http://localhost:8132/v1/publishers/google/models/gemini-3-flash:generateContent"

    # Headers
    headers = {
        "Authorization": "Bearer vk-vertex-app",  # â˜… Virtual Key à¸ˆà¸²à¸ Proxy
        "Content-Type": "application/json"
    }

    # Native Google JSON Payload
    payload = {
        "contents": [
            {
                "role": "user",
                "parts": [
                    {"text": "à¹€à¸‚à¸µà¸¢à¸™ Python function à¸—à¸µà¹ˆà¸«à¸²à¸•à¸±à¸§à¹€à¸¥à¸‚à¸—à¸µà¹ˆà¹ƒà¸«à¸à¹ˆà¸—à¸µà¹ˆà¸ªà¸¸à¸”"}
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.8,
            "maxOutputTokens": 1024,
            "topP": 0.95
        },
        # â˜… Thinking Config - à¸ªà¸³à¸«à¸£à¸±à¸š Gemini 3.0
        "thinkingConfig": {
            "type": "EXTENDED_THINKING",
            "budgetTokens": 5000
        }
    }

    print("ğŸ” Google Vertex AI via HTTP - Native JSON Example\n")
    print("ğŸ“¤ Sending request to Proxy...\n")

    try:
        # à¸ªà¹ˆà¸‡ POST request
        response = requests.post(url, headers=headers, json=payload)

        if response.status_code == 200:
            result = response.json()

            # à¹à¸ªà¸”à¸‡à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
            print(f"âœ… Status Code: {response.status_code}")
            
            if "candidates" in result and len(result["candidates"]) > 0:
                candidate = result["candidates"][0]
                
                # à¹à¸ªà¸”à¸‡ Thinking content à¸–à¹‰à¸²à¸¡à¸µ
                if "content" in candidate:
                    for part in candidate["content"].get("parts", []):
                        if "thinkingNote" in part:
                            print(f"\nğŸ’­ Extended Thinking:\n{part['thinkingNote']}\n")
                        if "text" in part:
                            print(f"ğŸ’¬ Response:\n{part['text']}")

                # à¹à¸ªà¸”à¸‡ Usage
                if "usageMetadata" in candidate:
                    usage = candidate["usageMetadata"]
                    print(f"\nğŸ“Š Usage:")
                    print(f"   - Prompt Tokens: {usage.get('promptTokenCount', 0)}")
                    print(f"   - Candidate Tokens: {usage.get('candidatesTokenCount', 0)}")
            else:
                print("No candidates in response")
                print(f"Full response: {json.dumps(result, indent=2)}")
        else:
            print(f"âŒ Error {response.status_code}")
            print(f"Response: {response.text}")

    except requests.exceptions.ConnectionError:
        print("âŒ Connection Error: à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ Proxy Server")
        print("ğŸ’¡ à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² Proxy Server à¸£à¸±à¸™à¸­à¸¢à¸¹à¹ˆ: ./llm-proxy serve")
    except Exception as e:
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()
